{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import mysql.connector\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoker\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('combined_data.csv')\n",
    "data['Rainfall.mm.'] = pd.to_numeric(data['Rainfall.mm.'], errors='coerce')  # Ensure Rainfall is numeric\n",
    "data = data.dropna(subset=['Rainfall.mm.'])\n",
    "data.head()\n",
    "\n",
    "\n",
    "# Features and Target\n",
    "X = data[['Initial_Year', 'Initial_Month', 'Forecast_Year', 'Foreast_Month', 'Province_ID']]\n",
    "y = data['Rainfall.mm.']\n",
    "\n",
    "# Scale the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "optimizer = Adam(learning_rate=0.0001, clipvalue=1.0)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Training iteration 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "--------------------------------------------------\n",
      "Training iteration 2/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "--------------------------------------------------\n",
      "Training iteration 3/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoker\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "----------------------------------------------------------------\n",
      "ค่าเฉลี่ยความน่าจะเป็นในการเกิดฝนหลังจาก 3 รอบ: 100.00%\n",
      "================================================================\n",
      "================================TESTING===============================\n",
      "Province ID: 1, Province Name: กรุงเทพมหานคร\n",
      "Province ID: 2, Province Name: จังหวัดกาญจนบุรี\n",
      "Province ID: 3, Province Name: จังหวัดการบุรีรัมย์\n",
      "Province ID: 4, Province Name: จังหวัดขอนแก่น\n",
      "Province ID: 5, Province Name: จังหวัดจันทบุรี\n",
      "Province ID: 6, Province Name: จังหวัดชลบุรี\n",
      "Province ID: 7, Province Name: จังหวัดชุมพร\n",
      "Province ID: 8, Province Name: จังหวัดเชียงราย\n",
      "Province ID: 9, Province Name: จังหวัดเชียงใหม่\n",
      "Province ID: 10, Province Name: จังหวัดตรัง\n",
      "Province ID: 11, Province Name: จังหวัดตราด\n",
      "Province ID: 12, Province Name: จังหวัดนครนายก\n",
      "Province ID: 13, Province Name: จังหวัดนครปฐม\n",
      "Province ID: 14, Province Name: จังหวัดนครราชสีมา\n",
      "Province ID: 15, Province Name: จังหวัดนครศรีธรรมราช\n",
      "Province ID: 16, Province Name: จังหวัดน่าน\n",
      "Province ID: 17, Province Name: จังหวัดบุรีรัมย์\n",
      "Province ID: 18, Province Name: จังหวัดปทุมธานี\n",
      "Province ID: 19, Province Name: จังหวัดพะเยา\n",
      "Province ID: 20, Province Name: จังหวัดพิษณุโลก\n",
      "Province ID: 21, Province Name: จังหวัดเพชรบุรี\n",
      "Province ID: 22, Province Name: จังหวัดเพชรบูรณ์\n",
      "Province ID: 23, Province Name: จังหวัดแม่ฮ่องสอน\n",
      "Province ID: 24, Province Name: จังหวัดลำปาง\n",
      "Province ID: 25, Province Name: จังหวัดลำพูน\n",
      "Province ID: 26, Province Name: จังหวัดเลย\n",
      "Province ID: 27, Province Name: จังหวัดศรีสะเกษ\n",
      "Province ID: 28, Province Name: จังหวัดสกลนคร\n",
      "Province ID: 29, Province Name: จังหวัดสงขลา\n",
      "Province ID: 30, Province Name: จังหวัดสุพรรณบุรี\n",
      "Province ID: 31, Province Name: จังหวัดสุราษฎร์ธานี\n",
      "Province ID: 32, Province Name: จังหวัดอำนาจเจริญ\n",
      "Province ID: 33, Province Name: จังหวัดอุดรธานี\n",
      "Province ID: 34, Province Name: จังหวัดอุตรดิตถ์\n",
      "Province ID: 35, Province Name: จังหวัดเชียงราย\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 4 features, but MinMaxScaler is expecting 5 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Predict using the model\u001b[39;00m\n\u001b[0;32m     80\u001b[0m input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[iY, iM, iD, city_id]])  \u001b[38;5;66;03m# Adjust as per feature requirements\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m input_data_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m result_probability \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(input_data_scaled)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===========================OUTPUT===============================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hoker\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\hoker\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:534\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    530\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    532\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 534\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    544\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n",
      "File \u001b[1;32mc:\\Users\\hoker\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\hoker\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 4 features, but MinMaxScaler is expecting 5 features as input."
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training parameters\n",
    "roun = 10  # Number of epochs\n",
    "batch_siz = 32  # Batch size\n",
    "train_round = 3  # Number of training iterations\n",
    "\n",
    "rain_probabilities = []\n",
    "\n",
    "for i in range(train_round):\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Training iteration {i+1}/{train_round}\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "    model.fit(X_scaled, y, epochs=roun, batch_size=batch_siz, verbose=0)\n",
    "    model.save(\"model.h5\")  # Save the model after training\n",
    "\n",
    "    # Prediction on the last day in the dataset\n",
    "    probability_last_day = model.predict(X_scaled[-1].reshape(1, -1))\n",
    "    rain_probabilities.append(probability_last_day[0][0] * 100)\n",
    "\n",
    "average_probability = sum(rain_probabilities) / len(rain_probabilities)\n",
    "print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(f\"ค่าเฉลี่ยความน่าจะเป็นในการเกิดฝนหลังจาก {train_round} รอบ: {average_probability:.2f}%\")\n",
    "print(\"================================================================\")\n",
    "\n",
    "# Testing section\n",
    "print(\"================================TESTING===============================\")\n",
    "provinces = [\n",
    "    (1, \"กรุงเทพมหานคร\"),\n",
    "    (2, \"จังหวัดกาญจนบุรี\"),\n",
    "    (3, \"จังหวัดการบุรีรัมย์\"),\n",
    "    (4, \"จังหวัดขอนแก่น\"),\n",
    "    (5, \"จังหวัดจันทบุรี\"),\n",
    "    (6, \"จังหวัดชลบุรี\"),\n",
    "    (7, \"จังหวัดชุมพร\"),\n",
    "    (8, \"จังหวัดเชียงราย\"),\n",
    "    (9, \"จังหวัดเชียงใหม่\"),\n",
    "    (10, \"จังหวัดตรัง\"),\n",
    "    (11, \"จังหวัดตราด\"),\n",
    "    (12, \"จังหวัดนครนายก\"),\n",
    "    (13, \"จังหวัดนครปฐม\"),\n",
    "    (14, \"จังหวัดนครราชสีมา\"),\n",
    "    (15, \"จังหวัดนครศรีธรรมราช\"),\n",
    "    (16, \"จังหวัดน่าน\"),\n",
    "    (17, \"จังหวัดบุรีรัมย์\"),\n",
    "    (18, \"จังหวัดปทุมธานี\"),\n",
    "    (19, \"จังหวัดพะเยา\"),\n",
    "    (20, \"จังหวัดพิษณุโลก\"),\n",
    "    (21, \"จังหวัดเพชรบุรี\"),\n",
    "    (22, \"จังหวัดเพชรบูรณ์\"),\n",
    "    (23, \"จังหวัดแม่ฮ่องสอน\"),\n",
    "    (24, \"จังหวัดลำปาง\"),\n",
    "    (25, \"จังหวัดลำพูน\"),\n",
    "    (26, \"จังหวัดเลย\"),\n",
    "    (27, \"จังหวัดศรีสะเกษ\"),\n",
    "    (28, \"จังหวัดสกลนคร\"),\n",
    "    (29, \"จังหวัดสงขลา\"),\n",
    "    (30, \"จังหวัดสุพรรณบุรี\"),\n",
    "    (31, \"จังหวัดสุราษฎร์ธานี\"),\n",
    "    (32, \"จังหวัดอำนาจเจริญ\"),\n",
    "    (33, \"จังหวัดอุดรธานี\"),\n",
    "    (34, \"จังหวัดอุตรดิตถ์\"),\n",
    "    (35, \"จังหวัดเชียงราย\"),\n",
    "]\n",
    "\n",
    "# Display provinces for user reference\n",
    "for province_id, province_name in provinces:\n",
    "    print(f\"Province ID: {province_id}, Province Name: {province_name}\")\n",
    "\n",
    "# Sample test date\n",
    "iD = 6  # Example day\n",
    "iM = 9  # Example month\n",
    "iY = 2024  # Example year\n",
    "city_id = 9  # Example province ID\n",
    "\n",
    "# Predict using the model\n",
    "input_data = np.array([[iY, iM, iD, city_id]])  # Adjust as per feature requirements\n",
    "input_data_scaled = scaler.transform(input_data)\n",
    "\n",
    "result_probability = model.predict(input_data_scaled)\n",
    "print(\"===========================OUTPUT===============================\")\n",
    "print(f\"ความน่าจะเป็นในการเกิดฝน: {result_probability[0][0] * 100:.2f}%\")\n",
    "print(\"================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
